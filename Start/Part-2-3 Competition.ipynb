{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "169986de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import cv2\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd7471a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(point, point2):\n",
    "    px, py = point\n",
    "    x, y = point2\n",
    "    return ((px - x) ** 2 + (py - y) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89499b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_use(people, phones):\n",
    "    phone_use = {}\n",
    "    for id, phone in phones.items():\n",
    "        distance = 1000000\n",
    "        for p_id, person in people.items():\n",
    "            new_distance = calculate_distance((person[\"bbox\"][0]+person[\"bbox\"][2]/2, person[\"bbox\"][1]+person[\"bbox\"][3]/2), (phone[\"bbox\"][0]+phone[\"bbox\"][2]/2, phone[\"bbox\"][1]+phone[\"bbox\"][3]/2))\n",
    "            if new_distance < distance:\n",
    "                distance = new_distance\n",
    "                phone_use[id] = p_id\n",
    "\n",
    "    return phone_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0be80bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation(results):\n",
    "    annotations = []\n",
    "    for result in results:\n",
    "        bbox = result[0], result[1], result[2], result[3]\n",
    "        certainty = result[4]\n",
    "        category = result[5]\n",
    "        area = result[0]*result[1] \n",
    "        annotation = {\"area\": area, \"bbox\": bbox, \"certainty\": certainty, \"category_id\": category}\n",
    "        annotations.append(annotation)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6617222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(image, label, bbox, color):\n",
    "    x, y, x2, y2 = bbox\n",
    "    x, y, x2, y2 = int(x), int(y), int(x2), int(y2)\n",
    "    cv2.rectangle(image, (x, y), (x2, y2), color, 2)\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    cv2.rectangle(image, (x, y - text_height - baseline), (x + text_width, y), color, thickness=cv2.FILLED)\n",
    "    cv2.putText(image, label, (x, y - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0dbcb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSENGER = (231, 48, 122)\n",
    "PASSENGER_USE = (4, 255, 1)\n",
    "HAND = (158, 124, 101)\n",
    "DRIVER = (126, 45, 34)\n",
    "DRIVER_USE = (0, 3, 252)\n",
    "\n",
    "def run_yolo(model,image_path):\n",
    "    \"\"\" \n",
    "    ids = {\n",
    "        CIH__id or hand: 0,\n",
    "        Cell_Mounted: 4,\n",
    "        face_id: 1,\n",
    "    }\n",
    "    \"\"\"\n",
    "    image_result = model(image_path)\n",
    "    image = cv2.imread(image_path)\n",
    "    #print(image_result.pred[0])\n",
    "\n",
    "    _, width, _ = image.shape\n",
    "\n",
    "    people = {}\n",
    "    phones = {}\n",
    "    hands = {}\n",
    "\n",
    "    annotations = extract_annotation(image_result.pred[0])\n",
    "    num_faces = 0\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if annotation['category_id'] == 1:\n",
    "            people[annotation[\"area\"]] = annotation\n",
    "            num_faces += 1\n",
    "        elif annotation[\"category_id\"] == 4 or annotation[\"category_id\"] == 0:\n",
    "            annotation['category_id'] = \"Phone\"\n",
    "            phones[annotation[\"area\"]] = annotation\n",
    "        elif annotation[\"category_id\"] == 2:\n",
    "            annotation['category_id'] = \"Hand\"\n",
    "            hands[annotation[\"area\"]] = annotation\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if annotation[\"category_id\"] == 1:\n",
    "            if annotation[\"area\"] < 5000 and num_faces > 1:\n",
    "                people[annotation[\"area\"]]['category_id'] = \"Passenger\"\n",
    "            elif annotation[\"bbox\"][0] < width / 2 and num_faces > 1:\n",
    "                people[annotation[\"area\"]]['category_id'] = \"Passenger\"\n",
    "            else:\n",
    "                people[annotation[\"area\"]]['category_id'] = \"Driver\"\n",
    "                driver = annotation[\"area\"]\n",
    "\n",
    "\n",
    "    phone_use_list = phone_use(people, phones)\n",
    "    for phone, person in phone_use_list.items():\n",
    "        if people[person][\"category_id\"] == \"Driver\":\n",
    "            phones[phone][\"category_id\"] = \"Driver Use\"\n",
    "        else:\n",
    "            phones[phone][\"category_id\"] = \"Passenger Use\"\n",
    "\n",
    "        \n",
    "    new_annotations = list(people.values()) + list(phones.values()) + list(hands.values())\n",
    "\n",
    "    driver_use = False\n",
    "    for ann in new_annotations:\n",
    "        if ann[\"category_id\"] == \"Driver\":\n",
    "            image = draw_label(image, ann[\"category_id\"], ann[\"bbox\"], DRIVER)\n",
    "        elif ann[\"category_id\"] == \"Passenger\":\n",
    "            image = draw_label(image, ann[\"category_id\"], ann[\"bbox\"], PASSENGER)\n",
    "        elif ann[\"category_id\"] == \"Driver Use\":\n",
    "            image = draw_label(image, ann[\"category_id\"], ann[\"bbox\"], DRIVER_USE)\n",
    "            driver_use = True\n",
    "        elif ann[\"category_id\"] == \"Passenger Use\":\n",
    "            image = draw_label(image, ann[\"category_id\"], ann[\"bbox\"], PASSENGER_USE)\n",
    "        elif ann[\"category_id\"] == \"Hand\":\n",
    "            image = draw_label(image, ann[\"category_id\"], ann[\"bbox\"], HAND)\n",
    "        \n",
    "\n",
    "    #image_result.save()\n",
    "    \n",
    "    return image, driver_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2730f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-330-gb20fa802 Python-3.11.9 torch-2.3.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20869098 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "\n",
    "# To run the model You just need to clone yolov5 repository from github \n",
    "# and add new_yolo_2.pt into yolov5\\models\\ directory\n",
    "# https://github.com/ultralytics/yolov5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = torch.hub.load(r\"yolov5\", 'custom', path=r\"yolov5\\models\\new_yolo_2.pt\", source='local') \n",
    "\n",
    "#model.conf = 0.50  # confidence threshold (0-1)\n",
    "#model.iou = 0.45  # NMS IoU threshold (0-1)  \n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dcb50c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 1 image to \u001b[1mruns\\detect\\exp14\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.72955e+02, 3.74971e+02, 8.87851e+02, 5.29964e+02, 9.69052e-01, 1.00000e+00],\n",
      "        [2.63338e+02, 3.40097e+02, 3.71212e+02, 4.74920e+02, 9.68198e-01, 1.00000e+00],\n",
      "        [6.29446e+02, 5.17787e+02, 7.42793e+02, 6.42067e+02, 8.59240e-01, 2.00000e+00],\n",
      "        [6.21837e+02, 4.32987e+02, 7.50511e+02, 6.05201e+02, 8.53580e-01, 0.00000e+00],\n",
      "        [1.11253e+03, 4.79347e+02, 1.20096e+03, 5.31686e+02, 8.25810e-01, 2.00000e+00],\n",
      "        [2.45718e+02, 4.41311e+02, 3.01816e+02, 5.39711e+02, 7.37672e-01, 2.00000e+00]])\n"
     ]
    }
   ],
   "source": [
    "result = run_yolo(model, 'PREPROCESSED_IMAGES/1/Image852.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe689e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"test.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8e917b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Class Names: [\"Cell Phone\", \"Face\", \"Hand\", \"OOD\", \"Cell Phone Attached\"]\n",
    "\n",
    "results = {}\n",
    "# this is sort of true the ids dont really mean this\n",
    "ids = {\n",
    "    \"CIH__id\": 0,\n",
    "    \"Cell\": 4,\n",
    "    \"face_id\": 1,\n",
    "    \"hand\": 2\n",
    "}\n",
    "\n",
    "\n",
    "status = [\"0\",\"1\"]\n",
    "labels,inputs = [], [] # not to sure\n",
    "folder_destination = \"PROCESSED/\"\n",
    "os.makedirs(folder_destination, exist_ok=True)\n",
    "for i,s in enumerate(status):\n",
    "    for index,image_path in enumerate(glob.glob(\"BLURRED_IMAGES\\\\\"+s+\"\\\\*\")):\n",
    "        img_name = os.path.basename(image_path)\n",
    "        processed_image, driver_use = run_yolo(model, image_path)\n",
    "        labels.append(driver_use)\n",
    "        inputs.append(processed_image)\n",
    "        if driver_use:\n",
    "            cv2.imwrite(folder_destination+\"1/\"+img_name, processed_image)\n",
    "        else:\n",
    "            cv2.imwrite(folder_destination+\"0/\"+img_name, processed_image)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eb4d7-ba42-4471-96c0-79a313001c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo_hand_Extraction(model,image_path,ids):\n",
    "    \"\"\"\n",
    "    ids = {\n",
    "        CIH__id: 0,\n",
    "        Cell: 4,\n",
    "        face_id: 1,\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    results = model(image_path)\n",
    "\n",
    "    \"\"\"\n",
    "    PART 3: FILL THIS AREA\n",
    "    \"\"\"\n",
    "        \n",
    "    return 0\n",
    "\n",
    "\n",
    "status = [\"1\",\"0\"]\n",
    "labels,inputs = [], []\n",
    "\n",
    "ids = {\n",
    "    \"CIH__id\": 0,\n",
    "    \"Cell\": 4,\n",
    "    \"face_id\": 1,\n",
    "}\n",
    "\n",
    "for i,s in enumerate(status):\n",
    "    for index,image_path in enumerate(glob.glob(\"BLURRED_IMAGES\\\\\"+s+\"\\\\*\")):\n",
    "        \"\"\"\n",
    "        PART 3: FILL THIS AREA\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa49ae-3c2e-4437-88b6-1f9ddfa467fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
