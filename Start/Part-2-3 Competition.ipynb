{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169986de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58bb7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link categories\n",
    "def link_categories(cat_id):\n",
    "    if cat_id == 1:\n",
    "        return \"Cell Phone\"\n",
    "    elif cat_id == 2:\n",
    "        return \"Face\"\n",
    "    elif cat_id == 3:\n",
    "        return \"Hand\"\n",
    "    elif cat_id == 4:\n",
    "        return \"OOD\"\n",
    "    elif cat_id == 5:\n",
    "        return \"Cell Phone Attached\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d848e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_annotations(image_name, coco):\n",
    "    image_id = None\n",
    "    for image in coco['images']:\n",
    "        if image['file_name'] == image_name:\n",
    "            image_id = image['id']\n",
    "            break\n",
    "    if image_id is None:\n",
    "        return []\n",
    "    return [annotation for annotation in coco['annotations'] if annotation['image_id'] == image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd7471a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(point, point2):\n",
    "    px, py = point\n",
    "    x, y = point2\n",
    "    return ((px - x) ** 2 + (py - y) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89499b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_use(people, phones):\n",
    "    phone_use = {}\n",
    "    for idx, phone in phones.items():\n",
    "        distance = 1000000\n",
    "        for P_idx, person in people.items():\n",
    "            new_distance = calculate_distance((person[\"bbox\"][0]+person[\"bbox\"][3]/2, person[\"bbox\"][1]+person[\"bbox\"][4]/2), (phone[\"bbox\"][0]+phone[\"bbox\"][3]/2, phone[\"bbox\"][1]+phone[\"bbox\"][4]/2))\n",
    "            if new_distance < distance:\n",
    "                distance = new_distance\n",
    "                phone_use[idx] = p_idx\n",
    "\n",
    "    return phone_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dbcb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo(model,image_path, coco):\n",
    "    \"\"\"\n",
    "    ids = {\n",
    "        CIH__id: 0,\n",
    "        Cell: 4,\n",
    "        face_id: 1,\n",
    "    }\n",
    "    \"\"\"\n",
    "    people = {}\n",
    "    driver = None\n",
    "    phones = {}\n",
    "    hands = {}\n",
    "    image_data = None\n",
    "    for image in coco['images']:\n",
    "        if image[\"file_name\"] == image_path:\n",
    "            image_data = image.copy()\n",
    "            break\n",
    "\n",
    "    annotations = lookup_annotations(image_path, coco)\n",
    "    num_faces = 0\n",
    "    phone_cnt = 0\n",
    "    hand_cnt = 0\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if annotation['category_id'] == 2:\n",
    "            people[num_faces] = annotation\n",
    "            num_faces += 1\n",
    "        elif annotation[\"category_id\"] == 3:\n",
    "            annotation['category_id'] = \"Hand\"\n",
    "            hands[hand_cnt] = annotation\n",
    "            hand_cnt += 1\n",
    "        elif annotation[\"category_id\"] == 1:\n",
    "            annotation['category_id'] = \"Phone\"\n",
    "            phones[phone_cnt]\n",
    "            phone_cnt += 1\n",
    "\n",
    "    phone_use_list = phone_use(people, phones)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if annotation[\"category_id\"] == 2:\n",
    "            if annotation[\"area\"] < 5000 and num_faces > 1:\n",
    "                annotation['category_id'] = \"Passenger\"\n",
    "            elif annotation[\"bbox\"][0] < image_data[\"width\"] / 2 and num_faces > 1:\n",
    "                annotation['category_id'] = \"Passenger\"\n",
    "            else:\n",
    "                annotation['category_id'] = \"Driver\"\n",
    "                driver = annotation\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    PASSENGER_COLOR = (122, 48, 231)\n",
    "    PASSENGER_USE = (1, 255, 4)\n",
    "    HAND = (101, 124, 158)\n",
    "    DRIVER = (34, 45, 126)\n",
    "    DRIVER_USE = (252, 3, 0)\n",
    "    THICKNESS = 2\n",
    "    FONT_SCALE = 0.5\n",
    "    \n",
    "    results = model(image_path)\n",
    "\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1)\n",
    "    cv2.rectangle(image, (x, y - text_height - baseline), (x + text_width, y), color, thickness=cv2.FILLED)\n",
    "    cv2.putText(image, label, (x, y - baseline), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    results.save()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2730f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-330-gb20fa802 Python-3.11.9 torch-2.3.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20869098 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "\n",
    "# To run the model You just need to clone yolov5 repository from github \n",
    "# and add new_yolo_2.pt into yolov5\\models\\ directory\n",
    "# https://github.com/ultralytics/yolov5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = torch.hub.load(r\"yolov5\", 'custom', path=r\"yolov5\\models\\new_yolo_2.pt\", source='local') \n",
    "\n",
    "#model.conf = 0.50  # confidence threshold (0-1)\n",
    "#model.iou = 0.45  # NMS IoU threshold (0-1)  \n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb50c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 1 image to \u001b[1mruns\\detect\\exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = run_yolo(model, 'PREPROCESSED_IMAGES/0/Image921.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe689e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[766.24011, 303.59766, 859.07654, 431.16425,   0.95058,   1.00000],\n",
       "        [687.71204, 468.83743, 783.04309, 541.87195,   0.90416,   2.00000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e917b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Class Names: [\"Cell Phone\", \"Face\", \"Hand\", \"OOD\", \"Cell Phone Attached\"]\n",
    "\n",
    "results = {}\n",
    "ids = {\n",
    "    \"CIH__id\": 0,\n",
    "    \"Cell\": 4,\n",
    "    \"face_id\": 1,\n",
    "}\n",
    "\n",
    "status = [\"0\",\"1\"]\n",
    "labels,inputs = [], []\n",
    "\n",
    "for i,s in enumerate(status):\n",
    "    for index,image_path in enumerate(glob.glob(\"BLURRED_IMAGES\\\\\"+s+\"\\\\*\")):\n",
    "        \"\"\"\n",
    "        PART 2: FILL THIS AREA\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eb4d7-ba42-4471-96c0-79a313001c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo_hand_Extraction(model,image_path,ids):\n",
    "    \"\"\"\n",
    "    ids = {\n",
    "        CIH__id: 0,\n",
    "        Cell: 4,\n",
    "        face_id: 1,\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    results = model(image_path)\n",
    "\n",
    "    \"\"\"\n",
    "    PART 3: FILL THIS AREA\n",
    "    \"\"\"\n",
    "        \n",
    "    return 0\n",
    "\n",
    "\n",
    "status = [\"1\",\"0\"]\n",
    "labels,inputs = [], []\n",
    "\n",
    "ids = {\n",
    "    \"CIH__id\": 0,\n",
    "    \"Cell\": 4,\n",
    "    \"face_id\": 1,\n",
    "}\n",
    "\n",
    "for i,s in enumerate(status):\n",
    "    for index,image_path in enumerate(glob.glob(\"BLURRED_IMAGES\\\\\"+s+\"\\\\*\")):\n",
    "        \"\"\"\n",
    "        PART 3: FILL THIS AREA\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa49ae-3c2e-4437-88b6-1f9ddfa467fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
